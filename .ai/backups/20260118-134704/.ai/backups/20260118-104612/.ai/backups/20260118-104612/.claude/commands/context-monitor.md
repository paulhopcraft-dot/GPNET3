# Context Monitor - Token Usage Tracking

## Purpose
Monitor current context size and optimize token usage in real-time.

## Usage
```bash
/context                    # Show current usage
/context --refresh         # Smart refresh with state preservation
/context --optimize        # Show optimization opportunities
```

## Implementation
Uses Task tool to spawn lightweight Haiku agent for context analysis:

### Context Size Estimation
```javascript
function estimateTokens(messages) {
  // Rough token estimation: ~4 chars per token
  const totalChars = messages.join('').length;
  return Math.ceil(totalChars / 4);
}
```

### Smart Refresh Strategy
```yaml
preserve_state:
  - current_project_path
  - active_todos
  - recent_file_changes
  - session_discoveries
  - optimization_settings
  - progress_tracking

refresh_triggers:
  - context_size > 85000
  - user_requests_refresh
  - before_complex_task
```

### Optimization Detection
```yaml
analyze_opportunities:
  - batching_potential: "3 git commands â†’ /smart-commit"
  - fast_path_routing: "5 file reads â†’ route to Haiku"
  - context_bloat: "Refresh recommended in 2 commands"
  - model_optimization: "Current task suitable for Haiku"
```

## Example Output
```bash
ðŸ“Š Context Usage Analysis

Current Status:
â”œâ”€â”€ Estimated tokens: 67,500 / 100,000 (67.5%)
â”œâ”€â”€ Status: Healthy (refresh in ~8 commands)
â”œâ”€â”€ Model: Sonnet (consider Haiku for simple tasks)
â””â”€â”€ Session cost: ~$3.25

ðŸ’¡ Optimization Opportunities:
â”œâ”€â”€ ðŸ”„ 3 pending git operations â†’ use /smart-commit (-60% tokens)
â”œâ”€â”€ âš¡ Next file read â†’ route to Haiku (-85% cost)
â”œâ”€â”€ ðŸ§¹ Context refresh optimal after current task
â””â”€â”€ ðŸ“ˆ Session savings: 73% vs baseline

Commands:
â”œâ”€â”€ /context --refresh    # Smart refresh now
â”œâ”€â”€ /context --optimize   # Detailed optimization analysis
â””â”€â”€ /smart-commit "msg"   # Batch git operations
```

## Integration
- Automatic monitoring of all commands
- Suggestion engine for optimization
- Seamless integration with model routing
- Real-time savings calculation